{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### First, clone the Github repository and get access to google drive"
      ],
      "metadata": {
        "id": "zRfnJ67nAJ1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ENTER YOUR MODEL\n",
        "#def model_object(nn_model_mit, X_train): #Just for NN\n",
        "def model_object():\n",
        "  my_model = KNeighborsClassifier()#nn_model_mit.build_regressor(n_train = len(X_train)) #For not NN: Enter the class name\n",
        "  return my_model"
      ],
      "metadata": {
        "id": "Tt9XD9BODaz7"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Give model a name before running the notebook:\n",
        "model_name = input(\"Enter name of model: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDtxAUfA4Aad",
        "outputId": "ceaba0f6-3e5f-4c53-dece-e05b9724bb5b"
      },
      "execution_count": 178,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter name of model: fourth_attempt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## First need to clone Github repo to access of other files\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "\n",
        "user = input('Github User name: ')\n",
        "password = getpass('Github Password: ')\n",
        "password = urllib.parse.quote(password) # your password is converted into url format\n",
        "#repo_name = 'human-protein-atlas-image-classification' #input('Repo name: ')\n",
        "\n",
        "cmd_string = 'git clone https://{0}:{1}@github.com/reneebrecht/human-protein-atlas-image-classification.git'.format(user, password)\n",
        "\n",
        "os.system(cmd_string)\n",
        "cmd_string, password = \"\", \"\" # removing the password from the variable \n",
        "\n",
        "# may also need to access google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uroId6DD_dLv",
        "outputId": "54e0b388-e940-4940-f1c5-b873b5aa52ff"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Github User name: Nilodnewg\n",
            "Github Password: ··········\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "project_id = ' imposing-league-354107'\n",
        "!gcloud config set project {project_id}"
      ],
      "metadata": {
        "id": "qnnwtbcBgZjq",
        "outputId": "739ed970-59ed-4667-8dc3-7727175459bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so that it finds the classes to import\n",
        "import sys\n",
        "sys.path.insert(0,'/content/human-protein-atlas-image-classification/notebooks')"
      ],
      "metadata": {
        "id": "i20BExqOBkd8"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fsspec # this is needed for pandas"
      ],
      "metadata": {
        "id": "zHfd49AaMPqN",
        "outputId": "ba257671-7f48-4c77-fcf0-53f355d5b79f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (2022.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now the main code begins..."
      ],
      "metadata": {
        "id": "vZoHb-RCA5Jd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "ZyFtpB_a_Q8P"
      },
      "outputs": [],
      "source": [
        "# Import all the libraries I need\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# ignore Deprecation Warning\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import sklearn.externals\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import joblib\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "\n",
        "from keras.models import Sequential # intitialize the ANN\n",
        "from keras.layers import Dense, Activation, Dropout      # create layers\n",
        "\n",
        "import pyarrow.parquet as pq\n",
        "import pyarrow as pa\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "np.random.seed(421)\n",
        "tf.random.set_seed(421)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gcsfs"
      ],
      "metadata": {
        "id": "GhKEEmE0fc0a",
        "outputId": "df5447a6-e365-43f0-c1d3-acc36b7b8140",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.7/dist-packages (2022.5.0)\n",
            "Requirement already satisfied: aiohttp<4 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (3.8.1)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (4.4.2)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (1.35.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gcsfs) (2.23.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.7/dist-packages (from gcsfs) (1.18.1)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from gcsfs) (0.4.6)\n",
            "Requirement already satisfied: fsspec==2022.5.0 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (2022.5.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4->gcsfs) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4->gcsfs) (2.1.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4->gcsfs) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4->gcsfs) (4.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4->gcsfs) (1.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4->gcsfs) (1.7.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4->gcsfs) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4->gcsfs) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4->gcsfs) (21.4.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (57.4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (4.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp<4->gcsfs) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (1.24.3)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->gcsfs) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->gcsfs) (0.4.1)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (1.31.6)\n",
            "Requirement already satisfied: protobuf<4.0.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (3.17.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (21.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (1.56.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (2022.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "IQ8SUWG1_Q8S"
      },
      "outputs": [],
      "source": [
        "#Ryans path\n",
        "#base_path = '/Users/ryan/neue_fische/human-protein-atlas-image-classification/data/'\n",
        "#embed_path = '/Users/ryan/neue_fische/human-protein-atlas-image-classification/data/embeddings_train/'\n",
        "\n",
        "#Gwens path\n",
        "#base_path = '../data/human-protein-atlas-image-classification_data/'\n",
        "#embed_path = '../data/human-protein-atlas-image-classification_data/embeddings_train/'\n",
        "\n",
        "# GClout bucket path\n",
        "base_path = 'gs://human_proteins/' # Why does this not work?\n",
        "embed_path = 'gs://human_proteins_data/embeddings_train/' # Why does this not work?\n",
        "\n",
        "# GDrive path \n",
        "#base_path = '/content/drive/MyDrive/Work/Courses/Neuefische/human-protein-atlas-image-classification/Data/'\n",
        "#embed_path = '/content/drive/MyDrive/Work/Courses/Neuefische/human-protein-atlas-image-classification/Data/embeddings_train/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "lFRhetkw_Q8S"
      },
      "outputs": [],
      "source": [
        "#Get classes from other notebooks\n",
        "from Helper_classes import Location_in_Target, Bin_Embedding, Prepared_Test_Train_Data, Prepare_NN_for_pipline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tjYYvqB_Q8T",
        "outputId": "37578363-ea38-45ff-82e6-cd1f8f36e47e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://human_proteins/train.csv...\n",
            "/ [0 files][    0.0 B/  1.2 MiB]                                                \r/ [1 files][  1.2 MiB/  1.2 MiB]                                                \r\n",
            "Operation completed over 1 objects/1.2 MiB.                                      \n"
          ]
        }
      ],
      "source": [
        "#get all of the labels\n",
        "# Download the file from a given Google Cloud Storage bucket.\n",
        "!gsutil cp 'gs://human_proteins/train.csv' /tmp/train.csv\n",
        "labels_training = pd.read_csv('/tmp/train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the embedding "
      ],
      "metadata": {
        "id": "JE-8Ug5tEwho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /tmp/embed_path\n",
        "!gsutil cp 'gs://human_proteins_data/embeddings_train/*' /tmp/embed_path"
      ],
      "metadata": {
        "id": "e1MR4praEwBV",
        "outputId": "5517711d-aaac-40bb-fbaf-5f939569db85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_0of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_10of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_11of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_12of32.parquet...\n",
            "- [4 files][ 22.0 MiB/ 22.0 MiB]                                                \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_13of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_14of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_15of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_16of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_17of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_18of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_19of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_1of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_20of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_21of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_22of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_23of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_24of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_25of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_26of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_27of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_28of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_29of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_2of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_30of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_31of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_3of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_4of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_5of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_6of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_7of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_8of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_9of32.parquet...\n",
            "\\ [32 files][171.3 MiB/171.3 MiB]                                               \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "\n",
            "Operation completed over 32 objects/171.3 MiB.                                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNiKFc_7_Q8U"
      },
      "source": [
        "#### Test the class with Mitochondria labeled with a 23\n",
        "Create a balanced subset of images that have mitochondria or not. <br>\n",
        "Get the embedded data for the mitochondria data and the same amount of data not labeled with mitochondria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "V1KTUoGX_Q8V"
      },
      "outputs": [],
      "source": [
        "def embedding_for_one_location(location_number):\n",
        "  mitochondria_pictures = Location_in_Target(location = location_number)\n",
        "  mitochondria_pictures.determine_pictures(labels_training)\n",
        "  bin_embed_mit = Bin_Embedding(mitochondria_pictures.get_pictures(), location_number, '/tmp/embed_path')\n",
        "  return mitochondria_pictures, bin_embed_mit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFZ8a6pe_Q8W"
      },
      "source": [
        "### Split into train/test and transform/standardize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "mgG922xK_Q8X"
      },
      "outputs": [],
      "source": [
        "def get_train_test(bin_embed_mit):\n",
        "  prepared_data_mit = Prepared_Test_Train_Data(bin_embed_mit.get_embedding())\n",
        "  X_train, X_test, y_train, y_test = prepared_data_mit.splitter()\n",
        "  return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "QyYTDFSd_Q8X"
      },
      "outputs": [],
      "source": [
        "#X_train.shape, y_train.shape, np.isnan(X_train).sum().sum(), y_train.isna().sum(), np.isnan(X_train).sum().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuXhaY23_Q8X"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "wHfJY9TK_Q8X"
      },
      "outputs": [],
      "source": [
        "def setting_up_nn():\n",
        "  #ONLY DO THIS FOR NN\n",
        "  #nn_model_mit = Prepare_NN_for_pipline()\n",
        "  #nn_model_mit.build_layers(number_layers=6, dropout_rate=0.25)\n",
        "  #plot_model(\n",
        "  #    nn_model_mit.model, to_file='model.png', show_shapes=True, \n",
        "  #    show_layer_names=True, dpi=96\n",
        "  #)\n",
        "  #return nn_model_mit\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "S4qJUH28_Q8Y"
      },
      "outputs": [],
      "source": [
        "def pipeline(my_model, X_train, y_train ):\n",
        "  with tf.device('/cpu:0'):\n",
        "\n",
        "    # just create the pipeline\n",
        "    pipe = make_pipeline(QuantileTransformer(random_state=0), StandardScaler(), my_model)\n",
        "    training = pipe.fit(X_train, y_train)  # apply scaling on training data\n",
        "  return pipe, training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "-B5y5511_Q8Y"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "#plt.plot(nn_model_mit.model.history.history['accuracy'])\n",
        "#plt.plot(nn_model_mit.model.history.history['val_accuracy'])\n",
        "#plt.title('model accuracy')\n",
        "#plt.ylabel('accuracy')\n",
        "#plt.xlabel('epoch')\n",
        "#plt.legend(['train', 'validation'], loc='upper left')\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZR4iMJH_Q8Y"
      },
      "source": [
        "### !!!! turning on regularizers seems to stall model, results don't improve just stuck under 0.5 for both train and validate. Running on test data shows it is only able to predict 1, never guesses 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "yE16iNE1_Q8Y"
      },
      "outputs": [],
      "source": [
        "def prediction_and_stuff(my_model, X_test, y_test, pipe):\n",
        "  #y_pred = my_model.model.predict(X_test)\n",
        "  y_pred = pipe.predict(X_test)\n",
        "\n",
        "  # Plotting the confusing matrix\n",
        "  mat = confusion_matrix(y_test, y_pred.round())\n",
        "  #sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
        "  #plt.xlabel('true label')\n",
        "  #plt.ylabel('predicted label');\n",
        "\n",
        "  #accuracy_score(y_test, y_pred.round()).round(2)\n",
        "  return y_pred, mat"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the results"
      ],
      "metadata": {
        "id": "QciXfrSX33Yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /tmp/saved_model"
      ],
      "metadata": {
        "id": "2chBbQO0IKy8"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "31WwDpCJ_Q8Z"
      },
      "outputs": [],
      "source": [
        "def save_one_model(my_model, y_pred, y_test, location_number, pipe):\n",
        "  #my_model.save('/tmp/saved_model/'+model_name+'_'+str(location_number))\n",
        "  joblib.dump([my_model, y_pred, y_test, pipe], '/tmp/saved_model/'+model_name+'_'+str(location_number)) \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for location_number in range(28):\n",
        "  mitochondria_pictures, bin_embed_mit = embedding_for_one_location(location_number)\n",
        "  X_train, X_test, y_train, y_test =  get_train_test(bin_embed_mit)\n",
        "  nn_model_mit = setting_up_nn()\n",
        "  my_model = model_object()\n",
        "  pipe, training =  pipeline(my_model, X_train, y_train )\n",
        "  y_pred, mat =  prediction_and_stuff(my_model, X_test, y_test, pipe)\n",
        "  save_one_model(my_model, y_pred, y_test, location_number, pipe)\n"
      ],
      "metadata": {
        "id": "mq2kXA6Q6kKh"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = '/tmp/saved_model/'+model_name+'*'\n",
        "!gsutil cp -r {filename} gs://human_proteins/saved_model/"
      ],
      "metadata": {
        "id": "IyAC_5ZFr4hn",
        "outputId": "6b0a6bd3-a698-4748-ff13-bfc2fed6d567",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file:///tmp/saved_model/fourth_attempt_1 [Content-Type=application/octet-stream]...\n",
            "Copying file:///tmp/saved_model/fourth_attempt_2 [Content-Type=application/octet-stream]...\n",
            "/ [2 files][208.1 MiB/208.1 MiB]                                                \n",
            "Operation completed over 2 objects/208.1 MiB.                                    \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.8 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "19f7bca3239b03b2de0c977c50cb26509aa4d9fb832a62e270228268f2f2cc6a"
      }
    },
    "colab": {
      "name": "SecondBinary_NN_improvement.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}