{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reneebrecht/human-protein-atlas-image-classification/blob/Yolobranch/3_Binary_models_CrossVal_and_GridSearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRfnJ67nAJ1k"
      },
      "source": [
        "### User input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRNmrn2oYU_v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# ENTER YOUR MODEL\n",
        "#def model_object(nn_model_mit, X_train): #Just for NN\n",
        "def model_object():\n",
        "  my_model = xgb.XGBClassifier(base_score=0.5, colsample_bylevel=1, \n",
        "       gamma=0, max_delta_step=0, missing=None, nthread=-1,\n",
        "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
        "       scale_pos_weight=1, seed=0, silent=True, random_state=42, subsample=1) # Can\n",
        "  \n",
        "  #my_model = AdaBoostClassifier() # Rene\n",
        "  #my_model = RandomForestClassifier() # Ryan\n",
        "  #my_model = ExtraTreesClassifier() # Bea\n",
        "  return my_model\n",
        "\n",
        "#Give model a name before running the notebook:\n",
        "model_name = 'ExtraTr_cv' #input(\"Enter name of model for saving: \") # xgboostclass\n",
        "\n",
        "# Define param grid. \n",
        "#If you do want to use a gridsearch, comment out the below param_grid = None \n",
        "# If you don't want to use a gridsearch, set param_grid = None .\n",
        "param_grid = {'model__n_estimators': [100, 150],\n",
        "              'model__learning_rate': [0.1], #np.arange(0, 0.30, 0.05)\n",
        "              'model__max_depth': [10], \n",
        "              'model__min_child_weight': [5, 7, 9] , \n",
        "              'model__colsample_bytree': [ 0.3, 0.4, 0.5 , 0.7 ],\n",
        "              'model__gamma': [1]\n",
        "              }\n",
        "#param_grid = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oquLCgc3i8Wn"
      },
      "source": [
        "### Setting up access to data and github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uroId6DD_dLv",
        "outputId": "385d618f-6f41-4cfd-e0df-8a4786092045"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Github User name: We-Cando\n",
            "Github Password: ··········\n",
            "Updated property [core/project].\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (2022.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.7/dist-packages (2022.5.0)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (1.35.0)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (4.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gcsfs) (2.23.0)\n",
            "Requirement already satisfied: fsspec==2022.5.0 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (2022.5.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.7/dist-packages (from gcsfs) (1.18.1)\n",
            "Requirement already satisfied: aiohttp<4 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (3.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from gcsfs) (0.4.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4->gcsfs) (21.4.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4->gcsfs) (4.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4->gcsfs) (4.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4->gcsfs) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4->gcsfs) (2.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4->gcsfs) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4->gcsfs) (6.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4->gcsfs) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4->gcsfs) (1.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (57.4.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (4.8)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp<4->gcsfs) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (2022.6.15)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->gcsfs) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->gcsfs) (0.4.1)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (1.31.6)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (21.3)\n",
            "Requirement already satisfied: protobuf<4.0.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (3.17.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (1.56.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (2022.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "## First need to clone Github repo to access necessary classes\n",
        "# -----------------------------\n",
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "import sys\n",
        "\n",
        "user = input('Github User name: ')\n",
        "password = getpass('Github Password: ')\n",
        "password = urllib.parse.quote(password) # your password is converted into url format\n",
        "#repo_name = 'human-protein-atlas-image-classification' #input('Repo name: ')\n",
        "\n",
        "cmd_string = 'git clone https://{0}:{1}@github.com/reneebrecht/human-protein-atlas-image-classification.git'.format(user, password)\n",
        "\n",
        "os.system(cmd_string)\n",
        "cmd_string, password = \"\", \"\" # removing the password from the variable \n",
        "\n",
        "# so that it finds the classes to import\n",
        "sys.path.insert(0,'/content/human-protein-atlas-image-classification/notebooks')\n",
        "\n",
        "## may also need to access google drive\n",
        "# -----------------------------\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "## get access to data on Google Cloud\n",
        "# -----------------------------\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "project_id = ' imposing-league-354107'\n",
        "!gcloud config set project {project_id}\n",
        "\n",
        "## install\n",
        "# -----------------------------\n",
        "!pip install fsspec # this is needed for pandas\n",
        "!pip install gcsfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZoHb-RCA5Jd"
      },
      "source": [
        "### Now the main code begins..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyFtpB_a_Q8P"
      },
      "outputs": [],
      "source": [
        "# Import all the libraries I need\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# ignore Deprecation Warning\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, make_scorer, fbeta_score\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import sklearn.externals\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import cross_validate, GridSearchCV\n",
        "import joblib\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "\n",
        "from keras.models import Sequential # intitialize the ANN\n",
        "from keras.layers import Dense, Activation, Dropout      # create layers\n",
        "\n",
        "import pyarrow.parquet as pq\n",
        "import pyarrow as pa\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "\n",
        "#Get classes from other notebooks\n",
        "from Helper_classes import Location_in_Target, Bin_Embedding, Prepared_Test_Train_Data, Prepare_NN_for_pipline\n",
        "\n",
        "np.random.seed(421)\n",
        "tf.random.set_seed(421)\n",
        "\n",
        "# GClout bucket path\n",
        "base_path = 'gs://human_proteins/' # Why does this not work?\n",
        "embed_path = 'gs://human_proteins_data/embeddings_train/' # Why does this not work?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mLSiRpejMgv"
      },
      "source": [
        "### Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tjYYvqB_Q8T",
        "outputId": "6753e27e-f630-4c10-c095-d8dba9806d42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying gs://human_proteins/train.csv...\n",
            "/ [0 files][    0.0 B/  1.2 MiB]                                                \r/ [1 files][  1.2 MiB/  1.2 MiB]                                                \r\n",
            "Operation completed over 1 objects/1.2 MiB.                                      \n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_0of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_10of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_11of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_12of32.parquet...\n",
            "- [4 files][ 22.0 MiB/ 22.0 MiB]                                                \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_13of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_14of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_15of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_16of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_17of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_18of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_19of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_1of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_20of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_21of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_22of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_23of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_24of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_25of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_26of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_27of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_28of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_29of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_2of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_30of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_31of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_3of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_4of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_5of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_6of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_7of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_8of32.parquet...\n",
            "Copying gs://human_proteins_data/embeddings_train/train_embeddings_9of32.parquet...\n",
            "/ [32 files][171.3 MiB/171.3 MiB]   14.6 MiB/s                                  \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "\n",
            "Operation completed over 32 objects/171.3 MiB.                                   \n"
          ]
        }
      ],
      "source": [
        "#get all of the labels\n",
        "# Download the file from a given Google Cloud Storage bucket.\n",
        "!gsutil cp 'gs://human_proteins/train.csv' /tmp/train.csv\n",
        "labels_training = pd.read_csv('/tmp/train.csv')\n",
        "\n",
        "# Get the embeddings\n",
        "!mkdir -p /tmp/embed_path\n",
        "!gsutil cp 'gs://human_proteins_data/embeddings_train/*' /tmp/embed_path\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE-8Ug5tEwho"
      },
      "source": [
        "#### Setup functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1KTUoGX_Q8V"
      },
      "outputs": [],
      "source": [
        "def skip_cross_val():\n",
        "    cross_val = input(\"Do you want to do a cross_validation? (y/n)\")\n",
        "    if cross_val.lower() in [\"y\", \"yes\"]:\n",
        "        cross_validation=True\n",
        "        print(\"Using a cross_validation\")\n",
        "    elif cross_val.lower() in [\"n\", \"no\"]:\n",
        "        cross_validation=False\n",
        "        print(\"Not using a cross_validation\")\n",
        "    else:\n",
        "        pass\n",
        "    return cross_validation\n",
        "\n",
        "def embedding_for_one_location(location_number):\n",
        "  \"\"\"Create a balanced subset of images that have a location or not.\n",
        "  Get all the embeddings for the chosen location and the same amount of embeddings without that location.\n",
        "  \"\"\"\n",
        "  mitochondria_pictures = Location_in_Target(location = location_number)\n",
        "  mitochondria_pictures.determine_pictures(labels_training)\n",
        "  bin_embed_mit = Bin_Embedding(mitochondria_pictures.get_pictures(), location_number, '/tmp/embed_path')\n",
        "  return mitochondria_pictures, bin_embed_mit\n",
        "\n",
        "def get_train_test(bin_embed_mit):\n",
        "  \"\"\"Split into train/test \"\"\" \n",
        "  prepared_data_mit = Prepared_Test_Train_Data(bin_embed_mit.get_embedding())\n",
        "  X_train, X_test, y_train, y_test = prepared_data_mit.splitter()\n",
        "  return X_train, X_test, y_train, y_test\n",
        "\n",
        "def run_cross_validate(clf, X_train, y_train):\n",
        "  \"\"\"running cross validation and fixing output\"\"\"\n",
        "  # Define fbeta score as the metric to compare the models\n",
        "  fbeta_scorer = make_scorer(fbeta_score, beta=1)\n",
        "  results = {}\n",
        "  scores = cross_validate(clf, X_train, y_train, scoring=fbeta_scorer, cv=5, n_jobs=-1, return_train_score=True)\n",
        "  #results = {key: [value.mean().round(4), value.std().round(4)] for key, value in scores.items()}\n",
        "  return scores\n",
        "\n",
        "def prediction_and_stuff(X_test, y_test, pipe_fitted):\n",
        "  \"\"\" predicting using the fitted model/pipeline and create confusion matrix \"\"\"\n",
        "  y_pred = pipe_fitted.predict(X_test)\n",
        "  # Plotting the confusion matrix\n",
        "  mat = confusion_matrix(y_test, y_pred.round())\n",
        "  return y_pred, mat\n",
        "\n",
        "def plot_train_vs_validate_score(cv_scores):\n",
        "  df_cv = pd.DataFrame(cv_scores)\n",
        "  df_cv.plot(y=['test_score', 'train_score'])\n",
        "  plt.xlabel('Fold')\n",
        "  plt.ylabel('F_beta score')\n",
        "  plt.ylim(0,1)\n",
        "  return df_cv\n",
        "\n",
        "# for saving\n",
        "!mkdir -p /tmp/saved_model\n",
        "\n",
        "def save_one_model(my_model, y_pred, y_test, y_pred_train, y_train, location_number, pipe_fitted, cv_scores):\n",
        "  joblib.dump([my_model, y_pred, y_test, y_pred_train, y_train, pipe_fitted], '/tmp/saved_model/'+model_name+'_'+str(location_number)) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaCiCUi4okTo"
      },
      "source": [
        "### Run everything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq2kXA6Q6kKh",
        "outputId": "88cf37b9-f0d2-4dbd-93d8-a9d59e158a2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25770\n",
            "Using a cross_validation\n",
            "----------------------------------------\n",
            "Model:  XGBClassifier(nthread=-1, random_state=42, seed=0, silent=True)\n",
            "Location:  0\n",
            "Fbeta     (train, test): 0.69 0.76\n",
            "----------------------------------------\n",
            "2508\n",
            "Using a cross_validation\n",
            "----------------------------------------\n",
            "Model:  XGBClassifier(nthread=-1, random_state=42, seed=0, silent=True)\n",
            "Location:  1\n",
            "Fbeta     (train, test): 0.68 0.95\n",
            "----------------------------------------\n",
            "7242\n",
            "Using a cross_validation\n",
            "----------------------------------------\n",
            "Model:  XGBClassifier(nthread=-1, random_state=42, seed=0, silent=True)\n",
            "Location:  2\n",
            "Fbeta     (train, test): 0.62 0.82\n",
            "----------------------------------------\n",
            "3122\n"
          ]
        }
      ],
      "source": [
        "for location_number in range(28):\n",
        "  location_pictures, bin_embed_loc = embedding_for_one_location(location_number)\n",
        "  print(bin_embed_loc.get_embedding().shape[0])\n",
        "  #if bin_embed_loc.get_embedding().shape[0] < 100: continue # only run for datasets > 100 images\n",
        "  X_train, X_test, y_train, y_test =  get_train_test(bin_embed_loc)\n",
        "  my_model = model_object()\n",
        "  # create pipeline\n",
        "  pipe = make_pipeline(QuantileTransformer(random_state=0), StandardScaler(), my_model)\n",
        "  \n",
        "  cross_validation=skip_cross_val()\n",
        "  if cross_validation == True:\n",
        "    # run cross validation and plot results\n",
        "    cv_scores = run_cross_validate(pipe, X_train, y_train)\n",
        "    df_cv = plot_train_vs_validate_score(cv_scores)\n",
        "  \n",
        "  if type(param_grid) is dict:\n",
        "    # Define fbeta score as the metric to compare the models\n",
        "    f_beta = 1\n",
        "    fbeta_scorer = make_scorer(fbeta_score, beta=f_beta)\n",
        "    gscv = GridSearchCV(pipe, param_grid, cv=5, verbose=1, n_jobs=-1, scoring=fbeta_scorer, return_train_score=True)\n",
        "\n",
        "  # train the model\n",
        "  pipe_fitted = pipe.fit(X_train, y_train) \n",
        "  # prediction on test and train data\n",
        "  y_pred, mat =  prediction_and_stuff(X_test, y_test, pipe_fitted)\n",
        "  y_pred_train, mat_train =  prediction_and_stuff(X_train, y_train, pipe_fitted)\n",
        "  print(\"----\"*10)\n",
        "  print('Model: ', my_model)\n",
        "  print('Location: ', location_number)\n",
        "  print(\"Fbeta     (train, test):\", fbeta_score(y_test, y_pred, beta=1).round(2), \n",
        "        fbeta_score(y_train, y_pred_train, beta=1).round(2))\n",
        "  print(\"----\"*10)\n",
        "  save_one_model(my_model, y_pred, y_test, y_pred_train, y_train, location_number, pipe_fitted, df_cv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyw59zMWonBQ"
      },
      "source": [
        "### Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyAC_5ZFr4hn"
      },
      "outputs": [],
      "source": [
        "filename = '/tmp/saved_model/'+model_name+'*'\n",
        "!gsutil cp -r {filename} gs://human_proteins/saved_model/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "3_Binary_models_CrossVal_and_GridSearch.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.8 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "a619ac50d6acfdd4302904b35e357ec76d80dfbd3ef5ddbc2025fd7ca8c4ef24"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}